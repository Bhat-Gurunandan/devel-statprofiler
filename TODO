- file output
  - decide whether to store the full sub/file name every time or
    cache recently-used ones to save on file size (and maybe I/O speed)
    I can imagine two ways of going about this:
    => A scheme like Sereal's COPY tag that could (in this case) appear
       in any context where a string is expected and that encodes an
       offset in the file from which the actual string is to be read.
       Sereal chooses absolute offsets for a number of reasons. This might
       be better off with relative backreferencing offsets. The upside of
       this is that the data producer can itself make trade-offs as to
       whether and where to make use of this. The decoder can start
       decoding from arbitrary positions in a file (though it may have
       to seek if it didn't read the entire file). The downside of this
       is that if we don't know anything about the data we're reading,
       in order to cache anything (to avoid lots of seeks), the decoder
       has to keep ALL strings in an in-memory cache because the encoder
       cannot realistically go back to flag strings that need keeping
       around (seek on encoder/profiler side is even worse).
    => More of an adaptive encoding where in order decode, one has to
       decode the entire file running up to the atom to be decoded
       because the state of the decoder ('s cached strings) is part of
       the decoding decisions.
    => I don't think these things are worth implementing.
  - use async I/O to reduce delays? (probably not)
- test various interesting cases
  - do ""
  - sort {}/map {}
- test under debugger
- differentiate BEGIN blocks/evals in profile visualization
- add option to set $^P to
  - 0x200 descriptive names for anonymous subroutines
  - 0x100 descriptive names for eval ""
  - 0x10  keep information about the definition line for a subroutine
  - 0x400 save source code for all files (optionally dumping it as soon
    as possible)
- command line options for analysis script
- calling stop -> enable DTRT
  - generate a new file id (child of the previous one)
  - check the file is reopened when re-entering the new runloop
- generated code containing #line directives
  - it might be useful to have both the original and generated code
    in the report, with some way to toggle between the two
- metadata/segmenting
  - for separate requests, needs to store per-request metadata
  - might store separate requests in separate files, or in separate
    segments of the same file
  - each request needs its own metadata
  - might be useful to have multiple different, potentially overlapping
    sub-segments (es. to mark a setup/teardown phase)
  - don't assume the metadata is available at the beginning of the request
  ===> Currently supporting a simple notion of sections. Let's see if that's
       good enough.
- some serious benchmarking
  - add a fast benchmark to be used during development and a slow more
    realistic one for release
- find a solution to correctly report exclusive/inclusive time for XSUBs
  with callbacks (might not be possible)

- report similar to NYTProf
  - for each line, compute % relative to sub
  - for each sub, compute % relative to total
  - for subs, % of samples aggregated by call site
- add heuristic to detect the real start of the sub (and default to
  first sampled line)
- flexible stack frame -> source code mapping
  - for eval, use the embedded source code
  - for Git repositories, fetch it from Git
  - for non-Git, fetch it from disk
  - for generated code, allow some custom mapping
- eval aggregation
- detect multiple eval with the same #line directive
- closure aggregation
- recursive sub aggregation
- would be nice to support arbitrary aggregation (es. by URL)
  - add post-process step that computes a per-segment summary, so
    aggregating different segments is hopefully faster
  - maybe do the aggregation directly in the browser, to reduce CPU
    usage on the storage server
- make aggregation code reusable both as a script and in a web application
- remove file text after __DATA__/__END__ but check it's not removing
  source code (use sampled lines)
- link source file listings from flame graph
- set default list of slow opcodes in bin/statprofilehtml
- file reader: Better error reporting
